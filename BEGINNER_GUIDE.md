# Kimi CLI ä»é›¶å¼€å§‹å®Œå…¨æŒæ¡æŒ‡å—

## ç›®å½•

1. [é¡¹ç›®æ¦‚è§ˆ](#1-é¡¹ç›®æ¦‚è§ˆ)
2. [ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
3. [æ ¸å¿ƒæ¦‚å¿µç†è§£](#3-æ ¸å¿ƒæ¦‚å¿µç†è§£)
4. [ä»£ç ç»“æ„æ·±åº¦è§£æ](#4-ä»£ç ç»“æ„æ·±åº¦è§£æ)
5. [ä»é›¶æ„å»º Coding Agent çš„æ ¸å¿ƒæŠ€æœ¯](#5-ä»é›¶æ„å»º-coding-agent-çš„æ ¸å¿ƒæŠ€æœ¯)
6. [å®æˆ˜ï¼šåŠ¨æ‰‹ä¿®æ”¹å’Œæ‰©å±•](#6-å®æˆ˜åŠ¨æ‰‹ä¿®æ”¹å’Œæ‰©å±•)
7. [é«˜çº§ä¸»é¢˜](#7-é«˜çº§ä¸»é¢˜)
8. [æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)

---

## 1. é¡¹ç›®æ¦‚è§ˆ

### 1.1 Kimi CLI æ˜¯ä»€ä¹ˆï¼Ÿ

Kimi CLI æ˜¯ Moonshot AI å¼€å‘çš„**äº¤äº’å¼å‘½ä»¤è¡Œ AI Agent**ï¼Œä¸“æ³¨äºè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„èŠå¤©æœºå™¨äººï¼Œè€Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿï¼š

- æ‰§è¡Œ Shell å‘½ä»¤
- è¯»å†™æ–‡ä»¶
- æœç´¢ä»£ç 
- è°ƒç”¨ Web API
- å§”æ´¾ä»»åŠ¡ç»™å­ Agent
- ç®¡ç†ä¼šè¯å†å²

çš„**å®Œæ•´ Agent ç³»ç»Ÿ**ã€‚

### 1.2 æŠ€æœ¯ç‰¹ç‚¹

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **è¯­è¨€** | Python 3.13+ |
| **æ¶æ„æ¨¡å¼** | æ¨¡å—åŒ–ã€æ’ä»¶å¼ã€äº‹ä»¶é©±åŠ¨ |
| **å¼‚æ­¥** | å…¨é¢ä½¿ç”¨ async/await |
| **å¯æ‰©å±•æ€§** | æ”¯æŒè‡ªå®šä¹‰å·¥å…·ã€Agentã€LLM æä¾›å•† |
| **åè®®æ”¯æŒ** | ACPã€MCPã€Wireï¼ˆå®éªŒæ€§ï¼‰ |
| **UI æ¨¡å¼** | Shellï¼ˆäº¤äº’å¼ï¼‰ã€Printï¼ˆéäº¤äº’ï¼‰ã€ACPï¼ˆIDE é›†æˆï¼‰ |

### 1.3 æ ¸å¿ƒæ•°æ®

- **ä»£ç é‡**ï¼š~2500 è¡Œ Python ä»£ç 
- **æ–‡ä»¶æ•°**ï¼š76 ä¸ª Python æ–‡ä»¶
- **æµ‹è¯•è¦†ç›–**ï¼š30+ æµ‹è¯•æ–‡ä»¶
- **å½“å‰ç‰ˆæœ¬**ï¼š0.58

---

## 2. ç¯å¢ƒå‡†å¤‡

### 2.1 å®‰è£…ä¾èµ–

```bash
# 1. å®‰è£… uvï¼ˆç°ä»£ Python åŒ…ç®¡ç†å™¨ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. å…‹éš†ä»“åº“
git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

# 3. å®‰è£…ä¾èµ–
make prepare  # ç­‰åŒäº uv sync --frozen
```

### 2.2 éªŒè¯å®‰è£…

```bash
# è¿è¡Œ Kimi CLI
uv run kimi --help

# è¿è¡Œæµ‹è¯•
make test

# ä»£ç æ£€æŸ¥
make check
```

### 2.3 IDE é…ç½®ï¼ˆæ¨è VS Codeï¼‰

å®‰è£…ä»¥ä¸‹æ‰©å±•ï¼š
- Python
- Pylance
- Ruff

`.vscode/settings.json`:
```json
{
  "python.languageServer": "Pylance",
  "python.analysis.typeCheckingMode": "basic",
  "[python]": {
    "editor.defaultFormatter": "charliermarsh.ruff",
    "editor.formatOnSave": true
  }
}
```

---

## 3. æ ¸å¿ƒæ¦‚å¿µç†è§£

### 3.1 ä»€ä¹ˆæ˜¯ Coding Agentï¼Ÿ

Coding Agent æ˜¯ä¸€ä¸ªèƒ½å¤Ÿç†è§£ç¼–ç¨‹ä»»åŠ¡ã€è‡ªä¸»è§„åˆ’ã€è°ƒç”¨å·¥å…·ã€æ‰§è¡Œä»£ç çš„ AI ç³»ç»Ÿã€‚

**æ ¸å¿ƒç»„æˆéƒ¨åˆ†**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          1. LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰             â”‚
â”‚     è´Ÿè´£ç†è§£ä»»åŠ¡ã€æ¨ç†ã€ç”Ÿæˆå“åº”           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       2. Agent æ‰§è¡Œå¼•æ“ï¼ˆSoulï¼‰           â”‚
â”‚     ç®¡ç†å¯¹è¯æµç¨‹ã€å·¥å…·è°ƒç”¨ã€é”™è¯¯å¤„ç†        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         3. å·¥å…·ç³»ç»Ÿï¼ˆToolsï¼‰              â”‚
â”‚   Shellã€æ–‡ä»¶æ“ä½œã€Webã€å­ Agent ç­‰        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         4. ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰                 â”‚
â”‚      Shellã€Printã€ACP ç­‰å¤šç§æ¨¡å¼          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Agent å·¥ä½œæµç¨‹

```
ç”¨æˆ·è¾“å…¥
   â†“
æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰
   â†“
è°ƒç”¨ LLMï¼ˆå¸¦ç³»ç»Ÿæç¤ºè¯ + å†å² + å·¥å…·å®šä¹‰ï¼‰
   â†“
LLM è¿”å›å“åº”
   â”œâ”€ çº¯æ–‡æœ¬ â†’ æ˜¾ç¤ºç»™ç”¨æˆ· â†’ ç»“æŸ
   â””â”€ å·¥å…·è°ƒç”¨ â†’ ç”¨æˆ·å®¡æ‰¹ â†’ æ‰§è¡Œå·¥å…· â†’ æ”¶é›†ç»“æœ â†’ ç»§ç»­å¾ªç¯
```

### 3.3 å…³é”®è®¾è®¡æ¨¡å¼

#### 3.3.1 ä¾èµ–æ³¨å…¥

å·¥å…·ä¸ç›´æ¥å¯¼å…¥ä¾èµ–ï¼Œè€Œæ˜¯é€šè¿‡ `Runtime` æ³¨å…¥ï¼š

```python
class ReadFile(Tool):
    async def _prepare(self, runtime: Runtime) -> None:
        self.kaos = runtime.kaos  # æ³¨å…¥æ–‡ä»¶ç³»ç»ŸæŠ½è±¡
        self.work_dir = runtime.work_dir
```

#### 3.3.2 åè®®é©±åŠ¨

å®šä¹‰æ¸…æ™°çš„æ¥å£åè®®ï¼š

```python
# KAOS åè®®ï¼ˆæ–‡ä»¶ç³»ç»ŸæŠ½è±¡ï¼‰
async def readtext(path: str) -> str: ...
async def writetext(path: str, content: str) -> None: ...

# Tool åè®®
class Tool:
    async def _prepare(self, runtime: Runtime) -> None: ...
    async def _execute(self, **params) -> Any: ...
```

#### 3.3.3 äº‹ä»¶é©±åŠ¨

ä½¿ç”¨è§‚å¯Ÿè€…æ¨¡å¼å¤„ç†äº‹ä»¶ï¼š

```python
# å·¥å…·æ‰§è¡Œäº‹ä»¶
self._emit(EventType.ToolCallDone, result)

# UI ç›‘å¬äº‹ä»¶
async def _on_text_delta(self, delta: str):
    self.console.print(delta, end="")
```

---

## 4. ä»£ç ç»“æ„æ·±åº¦è§£æ

### 4.1 å…¥å£æµç¨‹ï¼ˆä» `kimi` å‘½ä»¤å¼€å§‹ï¼‰

#### æ­¥éª¤ 1: CLI å…¥å£ (`cli.py:cli`)

```python
# src/kimi_cli/cli.py
@app.command()
def cli(
    model: str = typer.Option("kimi", "-m", "--model"),
    work_dir: Path = typer.Option(Path.cwd(), "-w", "--work-dir"),
    # ... æ›´å¤šå‚æ•°
):
    # 1. åŠ è½½é…ç½®
    config = load_config()

    # 2. åˆ›å»ºæˆ–æ¢å¤ä¼šè¯
    session = find_or_create_session(...)

    # 3. åˆå§‹åŒ–åº”ç”¨
    app = KimiCLI(...)

    # 4. è¿è¡Œå¯¹åº”æ¨¡å¼
    if print_mode:
        asyncio.run(app.run_print_mode(...))
    else:
        app.run_shell_mode()
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `typer` æ„å»º CLI
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
- ä¼šè¯ç®¡ç†ï¼ˆ`--continue` æ¢å¤ä¸Šæ¬¡ä¼šè¯ï¼‰

#### æ­¥éª¤ 2: åº”ç”¨åˆå§‹åŒ– (`app.py:KimiCLI`)

```python
# src/kimi_cli/app.py
class KimiCLI:
    def __init__(self, ...):
        # 1. åˆ›å»º LLM å®ä¾‹
        self.llm = create_llm(provider, model, ...)

        # 2. åˆ›å»º Runtimeï¼ˆä¾èµ–æ³¨å…¥å®¹å™¨ï¼‰
        self.runtime = Runtime(
            kaos=LocalKaos(),  # æ–‡ä»¶ç³»ç»ŸæŠ½è±¡
            work_dir=work_dir,
            llm=self.llm,
            ...
        )

        # 3. åŠ è½½ Agent é…ç½®
        self.agent = load_agent(agent_spec, runtime)

        # 4. è¿æ¥ MCP æœåŠ¡å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
        await self._connect_mcp_servers()

        # 5. åˆ›å»º Soulï¼ˆæ‰§è¡Œå¼•æ“ï¼‰
        self.soul = KimiSoul(
            agent=self.agent,
            context=self.context,
            runtime=self.runtime,
        )
```

**å…³é”®ç‚¹**ï¼š
- `Runtime` æ˜¯ä¾èµ–æ³¨å…¥å®¹å™¨
- `Agent` ä» YAML é…ç½®åŠ è½½
- `Soul` æ˜¯æ ¸å¿ƒæ‰§è¡Œå¼•æ“

#### æ­¥éª¤ 3: Soul æ‰§è¡Œ (`soul/kimisoul.py`)

```python
# src/kimi_cli/soul/kimisoul.py
class KimiSoul:
    async def step(self, user_message: str = None) -> StepResult:
        # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
        if user_message:
            self.context.add_user_message(user_message)

        # 2. è°ƒç”¨ LLM
        response = await self._call_llm_with_retry()

        # 3. å¤„ç†å“åº”
        if response.tool_calls:
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            results = await self._execute_tools(response.tool_calls)
            return StepResult(continue_loop=True)
        else:
            # è¿”å›æ–‡æœ¬å“åº”
            return StepResult(
                text=response.text,
                continue_loop=False
            )
```

**å…³é”®ç‚¹**ï¼š
- `step()` æ˜¯ Agent çš„ä¸€æ¬¡è¿­ä»£
- æ”¯æŒå·¥å…·è°ƒç”¨çš„å¾ªç¯æ‰§è¡Œ
- ä½¿ç”¨ `tenacity` å®ç°é‡è¯•æœºåˆ¶

### 4.2 æ ¸å¿ƒæ¨¡å—è¯¦è§£

#### 4.2.1 Agent ç³»ç»Ÿ (`agentspec.py` + `soul/agent.py`)

**Agent é…ç½®æ–‡ä»¶** (`agents/default/agent.yaml`):

```yaml
version: 1
agent:
  name: "Kimi CLI Agent"
  system_prompt_path: ./system.md
  system_prompt_args:
    ROLE_ADDITIONAL: ""
  tools:
    - "kimi_cli.tools.shell:Shell"
    - "kimi_cli.tools.file:ReadFile"
    - "kimi_cli.tools.file:WriteFile"
    # ... æ›´å¤šå·¥å…·
  subagents:
    coder:
      path: ./sub.yaml
      description: "Good at general software engineering tasks."
```

**ç³»ç»Ÿæç¤ºè¯** (`agents/default/system.md`):

```markdown
You are Kimi CLI, an AI coding assistant.

Current time: ${KIMI_NOW}
Working directory: ${KIMI_WORK_DIR}

Directory contents:
${KIMI_WORK_DIR_LS}

You have access to the following tools:
- Shell: Execute shell commands
- ReadFile: Read file contents
- ...
```

**åŠ è½½è¿‡ç¨‹**:

```python
def load_agent(spec_path: str, runtime: Runtime) -> Agent:
    # 1. è§£æ YAML
    spec = AgentSpec.from_yaml(spec_path)

    # 2. æ¸²æŸ“ç³»ç»Ÿæç¤ºè¯ï¼ˆæ›¿æ¢å˜é‡ï¼‰
    system_prompt = render_system_prompt(
        spec.system_prompt_path,
        spec.system_prompt_args,
        runtime,
    )

    # 3. åŠ è½½å·¥å…·
    tools = []
    for tool_ref in spec.tools:
        tool_class = import_tool(tool_ref)
        tool = tool_class()
        await tool._prepare(runtime)  # ä¾èµ–æ³¨å…¥
        tools.append(tool)

    # 4. åŠ è½½å­ Agent
    subagents = load_subagents(spec.subagents, runtime)

    return Agent(
        name=spec.name,
        system_prompt=system_prompt,
        tools=tools,
        subagents=subagents,
    )
```

#### 4.2.2 å·¥å…·ç³»ç»Ÿ (`tools/`)

**å·¥å…·æ¥å£**ï¼ˆåŸºäº `kosong` æ¡†æ¶ï¼‰:

```python
from kosong import Tool

class ReadFile(Tool):
    """å·¥å…·å¿…é¡»ç»§æ‰¿ kosong.Tool"""

    # 1. å·¥å…·æè¿°ï¼ˆMarkdown æ ¼å¼ï¼‰
    @property
    def description(self) -> str:
        return """
        Read the contents of a file.

        Parameters:
        - file_path (str): Path to the file
        - offset (int): Starting line number
        - limit (int): Number of lines to read
        """

    # 2. å‚æ•° Schemaï¼ˆJSON Schemaï¼‰
    @property
    def parameters(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "file_path": {"type": "string"},
                "offset": {"type": "integer", "default": 0},
                "limit": {"type": "integer", "default": -1},
            },
            "required": ["file_path"],
        }

    # 3. ä¾èµ–æ³¨å…¥ï¼ˆå¯é€‰ï¼‰
    async def _prepare(self, runtime: Runtime) -> None:
        self.kaos = runtime.kaos
        self.work_dir = runtime.work_dir

    # 4. æ‰§è¡Œé€»è¾‘
    async def _execute(
        self,
        file_path: str,
        offset: int = 0,
        limit: int = -1,
    ) -> str:
        # å®ç°æ–‡ä»¶è¯»å–é€»è¾‘
        full_path = self.work_dir / file_path
        content = await self.kaos.readtext(full_path)

        # å¤„ç†è¡Œå·é™åˆ¶
        lines = content.splitlines()
        if limit > 0:
            lines = lines[offset:offset+limit]

        # æ ¼å¼åŒ–è¾“å‡ºï¼ˆå¸¦è¡Œå·ï¼‰
        return "\n".join(
            f"{i+1:>5} {line}"
            for i, line in enumerate(lines, start=offset)
        )
```

**å·¥å…·è°ƒç”¨æµç¨‹**:

```
LLM è¿”å›å·¥å…·è°ƒç”¨
   â†“
Soul è§£æå·¥å…·åç§°å’Œå‚æ•°
   â†“
Approval ç³»ç»Ÿæ£€æŸ¥ï¼ˆé YOLO æ¨¡å¼ï¼‰
   â†“
DenwaRenji åè°ƒæ‰§è¡Œ
   â†“
å·¥å…· _execute() æ–¹æ³•æ‰§è¡Œ
   â†“
è¿”å›ç»“æœæˆ–é”™è¯¯
   â†“
ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
   â†“
ç»§ç»­ä¸‹ä¸€è½® LLM è°ƒç”¨
```

**å®æˆ˜ç¤ºä¾‹ï¼šåˆ›å»ºæ–°å·¥å…·**

åˆ›å»ºä¸€ä¸ªè®¡ç®—æ–‡ä»¶è¡Œæ•°çš„å·¥å…·ï¼š

```python
# src/kimi_cli/tools/file/count_lines.py
from kosong import Tool
from kimi_cli.soul.agent import Runtime

class CountLines(Tool):
    @property
    def description(self) -> str:
        return "Count the number of lines in a file."

    @property
    def parameters(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "file_path": {"type": "string", "description": "Path to the file"},
            },
            "required": ["file_path"],
        }

    async def _prepare(self, runtime: Runtime) -> None:
        self.kaos = runtime.kaos
        self.work_dir = runtime.work_dir

    async def _execute(self, file_path: str) -> dict:
        full_path = self.work_dir / file_path
        content = await self.kaos.readtext(full_path)
        line_count = len(content.splitlines())

        return {
            "file_path": file_path,
            "line_count": line_count,
        }
```

ç„¶ååœ¨ Agent é…ç½®ä¸­æ·»åŠ ï¼š

```yaml
tools:
  - "kimi_cli.tools.file:CountLines"
```

#### 4.2.3 KAOS æ–‡ä»¶ç³»ç»ŸæŠ½è±¡ (`kaos/`)

**è®¾è®¡ç›®çš„**ï¼š
- ç»Ÿä¸€æœ¬åœ°å’Œè¿œç¨‹æ–‡ä»¶ç³»ç»Ÿæ¥å£
- ä¸ºæœªæ¥çš„æ²™ç®±ç¯å¢ƒåšå‡†å¤‡
- ç®€åŒ–æµ‹è¯•ï¼ˆå¯ä»¥ Mockï¼‰

**æ ¸å¿ƒæ¥å£**:

```python
# src/kaos/__init__.py

# Context variableï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
_kaos_instance: ContextVar[Kaos] = ContextVar("kaos")

# å¼‚æ­¥æ–‡ä»¶æ“ä½œ
async def readtext(path: str | Path) -> str:
    kaos = _kaos_instance.get()
    return await kaos.readtext(path)

async def writetext(path: str | Path, content: str) -> None:
    kaos = _kaos_instance.get()
    await kaos.writetext(path, content)

async def exists(path: str | Path) -> bool:
    kaos = _kaos_instance.get()
    return await kaos.exists(path)

async def iterdir(path: str | Path) -> AsyncIterator[Path]:
    kaos = _kaos_instance.get()
    async for p in kaos.iterdir(path):
        yield p
```

**æœ¬åœ°å®ç°**:

```python
# src/kaos/local.py
class LocalKaos(Kaos):
    async def readtext(self, path: Path) -> str:
        async with aiofiles.open(path, "r") as f:
            return await f.read()

    async def writetext(self, path: Path, content: str) -> None:
        async with aiofiles.open(path, "w") as f:
            await f.write(content)

    async def exists(self, path: Path) -> bool:
        return path.exists()

    async def iterdir(self, path: Path) -> AsyncIterator[Path]:
        for item in path.iterdir():
            yield item
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
import kaos

# åœ¨ Runtime ä¸­è®¾ç½®
kaos.set_kaos(LocalKaos())

# åœ¨å·¥å…·ä¸­ä½¿ç”¨
content = await kaos.readtext("/path/to/file")
await kaos.writetext("/path/to/file", "new content")

async for path in kaos.iterdir("/path/to/dir"):
    print(path)
```

#### 4.2.4 UI ç³»ç»Ÿ (`ui/`)

**Shell æ¨¡å¼** (`ui/shell/__init__.py`):

```python
class ShellApp:
    def __init__(self, soul: KimiSoul, ...):
        self.soul = soul
        self.session = PromptSession(
            completer=KimiCompleter(),  # è‡ªåŠ¨è¡¥å…¨
            key_bindings=create_key_bindings(),  # å¿«æ·é”®
            style=create_style(),  # æ ·å¼
        )

    def run(self):
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = self.session.prompt("> ")

                # å¤„ç†å…ƒå‘½ä»¤
                if user_input.startswith("/"):
                    self._handle_meta_command(user_input)
                    continue

                # Agent æ‰§è¡Œ
                async for event in self.soul.step(user_input):
                    if event.type == EventType.TextDelta:
                        self.console.print(event.data, end="")
                    elif event.type == EventType.ToolCall:
                        self._display_tool_call(event.data)

            except KeyboardInterrupt:
                continue
            except EOFError:
                break
```

**å…³é”®ç‰¹æ€§**ï¼š
- åŸºäº `prompt-toolkit` æ„å»º
- æ”¯æŒ Ctrl-X åˆ‡æ¢ Agent/Shell æ¨¡å¼
- å®æ—¶æµå¼è¾“å‡º
- Markdown æ¸²æŸ“

**Print æ¨¡å¼** (`ui/print/__init__.py`):

```python
async def run_print_mode(
    soul: KimiSoul,
    user_message: str,
    output_format: str = "text",
):
    # éäº¤äº’æ¨¡å¼ï¼Œç›´æ¥æ‰§è¡Œ
    result = await soul.step(user_message)

    if output_format == "text":
        print(result.text)
    elif output_format == "stream-json":
        # è¾“å‡º JSON æ ¼å¼ï¼ˆé€‚åˆè§£æï¼‰
        print(json.dumps({
            "text": result.text,
            "tool_calls": result.tool_calls,
        }))
```

---

## 5. ä»é›¶æ„å»º Coding Agent çš„æ ¸å¿ƒæŠ€æœ¯

### 5.1 æ ¸å¿ƒæŠ€æœ¯æ ˆ

#### 5.1.1 LLM é›†æˆï¼ˆä»¥ OpenAI ä¸ºä¾‹ï¼‰

```python
from openai import AsyncOpenAI

class LLMProvider:
    def __init__(self, api_key: str, base_url: str):
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
        )

    async def chat(
        self,
        messages: list[dict],
        tools: list[dict] = None,
        model: str = "gpt-4",
    ) -> dict:
        response = await self.client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice="auto",  # è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
        )

        message = response.choices[0].message

        return {
            "text": message.content,
            "tool_calls": [
                {
                    "id": tc.id,
                    "name": tc.function.name,
                    "parameters": json.loads(tc.function.arguments),
                }
                for tc in message.tool_calls or []
            ],
        }
```

#### 5.1.2 å·¥å…·ç³»ç»Ÿè®¾è®¡

**æ ¸å¿ƒåŸåˆ™**ï¼š
1. **å£°æ˜å¼**ï¼šå·¥å…·é€šè¿‡ Schema æè¿°è‡ªå·±
2. **å¼‚æ­¥**ï¼šæ”¯æŒ IO å¯†é›†å‹æ“ä½œ
3. **éš”ç¦»**ï¼šå·¥å…·ä¹‹é—´äº’ä¸ä¾èµ–
4. **å¯ç»„åˆ**ï¼šé€šè¿‡ Runtime å…±äº«èµ„æº

**ç¤ºä¾‹ï¼šShell å·¥å…·**

```python
import asyncio
import subprocess

class ShellTool(Tool):
    @property
    def description(self) -> str:
        return """
        Execute a shell command and return the output.

        **WARNING**: Be careful with shell commands!
        """

    @property
    def parameters(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "command": {"type": "string"},
                "timeout": {"type": "integer", "default": 30},
            },
            "required": ["command"],
        }

    async def _execute(self, command: str, timeout: int = 30) -> dict:
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )

            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=timeout,
            )

            return {
                "stdout": stdout.decode(),
                "stderr": stderr.decode(),
                "exit_code": process.returncode,
            }
        except asyncio.TimeoutError:
            process.kill()
            raise ToolExecutionError(f"Command timed out after {timeout}s")
```

#### 5.1.3 ä¸Šä¸‹æ–‡ç®¡ç†

**è®¾è®¡è¦ç‚¹**ï¼š
- ç®¡ç†å¯¹è¯å†å²
- å‹ç¼©é•¿ä¸Šä¸‹æ–‡
- æŒä¹…åŒ–ä¼šè¯

```python
class Context:
    def __init__(self, max_tokens: int = 100000):
        self.messages: list[dict] = []
        self.max_tokens = max_tokens

    def add_user_message(self, text: str):
        self.messages.append({
            "role": "user",
            "content": text,
        })

    def add_assistant_message(self, text: str, tool_calls: list = None):
        self.messages.append({
            "role": "assistant",
            "content": text,
            "tool_calls": tool_calls,
        })

    def add_tool_result(self, tool_call_id: str, result: str):
        self.messages.append({
            "role": "tool",
            "tool_call_id": tool_call_id,
            "content": result,
        })

    async def compact_if_needed(self, llm: LLMProvider):
        # ä¼°ç®— token æ•°é‡
        total_tokens = self._estimate_tokens()

        if total_tokens > self.max_tokens * 0.8:
            # å‹ç¼©å†å²
            compressed = await self._compress_history(llm)
            self.messages = [
                self.messages[0],  # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
                {"role": "system", "content": f"Previous context: {compressed}"},
                *self.messages[-10:],  # ä¿ç•™æœ€è¿‘ 10 æ¡
            ]

    async def _compress_history(self, llm: LLMProvider) -> str:
        # è°ƒç”¨ LLM å‹ç¼©å†å²
        response = await llm.chat([
            {"role": "system", "content": "Summarize the following conversation:"},
            *self.messages[1:-10],
        ])
        return response["text"]

    def save(self, path: str):
        with open(path, "w") as f:
            json.dump(self.messages, f, indent=2)

    @classmethod
    def load(cls, path: str) -> "Context":
        with open(path, "r") as f:
            messages = json.load(f)
        context = cls()
        context.messages = messages
        return context
```

#### 5.1.4 æ‰§è¡Œå¼•æ“

**æ ¸å¿ƒå¾ªç¯**:

```python
class AgentEngine:
    def __init__(
        self,
        llm: LLMProvider,
        tools: list[Tool],
        context: Context,
    ):
        self.llm = llm
        self.tools = {tool.name: tool for tool in tools}
        self.context = context

    async def step(self, user_message: str = None) -> dict:
        # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        if user_message:
            self.context.add_user_message(user_message)

        # 2. å‡†å¤‡å·¥å…·å®šä¹‰
        tool_schemas = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.parameters,
                }
            }
            for tool in self.tools.values()
        ]

        # 3. è°ƒç”¨ LLM
        response = await self.llm.chat(
            messages=self.context.messages,
            tools=tool_schemas,
        )

        # 4. å¤„ç†å“åº”
        if response["tool_calls"]:
            # æ‰§è¡Œå·¥å…·
            tool_results = await self._execute_tools(response["tool_calls"])

            # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            self.context.add_assistant_message(
                text=response["text"],
                tool_calls=response["tool_calls"],
            )
            for tc, result in zip(response["tool_calls"], tool_results):
                self.context.add_tool_result(tc["id"], result)

            # ç»§ç»­å¾ªç¯
            return await self.step()
        else:
            # è¿”å›æœ€ç»ˆå“åº”
            self.context.add_assistant_message(response["text"])
            return response

    async def _execute_tools(self, tool_calls: list[dict]) -> list[str]:
        results = []
        for tc in tool_calls:
            tool = self.tools[tc["name"]]
            try:
                result = await tool._execute(**tc["parameters"])
                results.append(json.dumps(result))
            except Exception as e:
                results.append(f"Error: {str(e)}")
        return results
```

### 5.2 å…³é”®è®¾è®¡å†³ç­–

#### 5.2.1 ä¸ºä»€ä¹ˆä½¿ç”¨å¼‚æ­¥ï¼Ÿ

**é—®é¢˜**ï¼šAgent éœ€è¦æ‰§è¡Œå¤šä¸ª IO æ“ä½œï¼ˆæ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ã€Shell å‘½ä»¤ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ `async/await` å®ç°å¹¶å‘

```python
# åŒæ­¥ç‰ˆæœ¬ï¼ˆæ…¢ï¼‰
def execute_tools(tool_calls):
    results = []
    for tc in tool_calls:
        result = tool.execute(tc)  # é˜»å¡
        results.append(result)
    return results

# å¼‚æ­¥ç‰ˆæœ¬ï¼ˆå¿«ï¼‰
async def execute_tools(tool_calls):
    tasks = [
        tool.execute(tc)
        for tc in tool_calls
    ]
    results = await asyncio.gather(*tasks)  # å¹¶è¡Œæ‰§è¡Œ
    return results
```

#### 5.2.2 ä¸ºä»€ä¹ˆéœ€è¦ KAOSï¼Ÿ

**é—®é¢˜**ï¼š
- å·¥å…·ç›´æ¥è®¿é—®æ–‡ä»¶ç³»ç»Ÿä¸å®‰å…¨
- éš¾ä»¥æµ‹è¯•ï¼ˆéœ€è¦çœŸå®æ–‡ä»¶ï¼‰
- æ— æ³•æ”¯æŒè¿œç¨‹æ–‡ä»¶ç³»ç»Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼šå¼•å…¥æŠ½è±¡å±‚

```python
# ä¸å¥½çš„è®¾è®¡
class ReadFile(Tool):
    async def _execute(self, file_path: str):
        with open(file_path) as f:  # ç›´æ¥è®¿é—®
            return f.read()

# å¥½çš„è®¾è®¡
class ReadFile(Tool):
    async def _prepare(self, runtime: Runtime):
        self.kaos = runtime.kaos  # ä¾èµ–æ³¨å…¥

    async def _execute(self, file_path: str):
        return await self.kaos.readtext(file_path)  # é€šè¿‡æŠ½è±¡å±‚
```

#### 5.2.3 ä¸ºä»€ä¹ˆéœ€è¦å®¡æ‰¹æœºåˆ¶ï¼Ÿ

**é—®é¢˜**ï¼šAgent å¯èƒ½æ‰§è¡Œå±é™©æ“ä½œï¼ˆåˆ é™¤æ–‡ä»¶ã€è¿è¡Œ `rm -rf`ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨å·¥å…·æ‰§è¡Œå‰è¯¢é—®ç”¨æˆ·

```python
class ApprovalSystem:
    async def approve_tool_call(self, tool_call: dict) -> bool:
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯
        print(f"Agent wants to call: {tool_call['name']}")
        print(f"Parameters: {tool_call['parameters']}")

        # è¯¢é—®ç”¨æˆ·
        response = input("Approve? [y/n]: ")
        return response.lower() == "y"
```

### 5.3 å®æˆ˜ï¼šæ„å»ºä¸€ä¸ªæœ€å° Agent

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€åŒ–ç‰ˆçš„ Agentï¼š

```python
# mini_agent.py
import asyncio
import json
from openai import AsyncOpenAI

class MiniAgent:
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)
        self.messages = []
        self.tools = {
            "read_file": self._read_file,
            "write_file": self._write_file,
        }

    async def chat(self, user_input: str):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({
            "role": "user",
            "content": user_input,
        })

        # å®šä¹‰å·¥å…·
        tool_schemas = [
            {
                "type": "function",
                "function": {
                    "name": "read_file",
                    "description": "Read a file",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {"type": "string"},
                        },
                        "required": ["path"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "write_file",
                    "description": "Write to a file",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {"type": "string"},
                            "content": {"type": "string"},
                        },
                        "required": ["path", "content"],
                    },
                },
            },
        ]

        # è°ƒç”¨ OpenAI
        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=self.messages,
            tools=tool_schemas,
        )

        message = response.choices[0].message

        # å¤„ç†å·¥å…·è°ƒç”¨
        if message.tool_calls:
            self.messages.append({
                "role": "assistant",
                "content": message.content,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments,
                        },
                    }
                    for tc in message.tool_calls
                ],
            })

            for tc in message.tool_calls:
                tool_name = tc.function.name
                args = json.loads(tc.function.arguments)
                result = await self.tools[tool_name](**args)

                self.messages.append({
                    "role": "tool",
                    "tool_call_id": tc.id,
                    "content": result,
                })

            # ç»§ç»­å¯¹è¯
            return await self.chat("")
        else:
            # è¿”å›æœ€ç»ˆå“åº”
            self.messages.append({
                "role": "assistant",
                "content": message.content,
            })
            return message.content

    async def _read_file(self, path: str) -> str:
        try:
            with open(path) as f:
                return f.read()
        except Exception as e:
            return f"Error: {str(e)}"

    async def _write_file(self, path: str, content: str) -> str:
        try:
            with open(path, "w") as f:
                f.write(content)
            return "File written successfully"
        except Exception as e:
            return f"Error: {str(e)}"

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    agent = MiniAgent(api_key="sk-...")

    response = await agent.chat("Read the file README.md and tell me what it says")
    print(response)

asyncio.run(main())
```

---

## 6. å®æˆ˜ï¼šåŠ¨æ‰‹ä¿®æ”¹å’Œæ‰©å±•

### 6.1 ä»»åŠ¡ 1ï¼šæ·»åŠ ä¸€ä¸ªæ–°å·¥å…· - æ–‡ä»¶æœç´¢

**ç›®æ ‡**ï¼šå®ç°ä¸€ä¸ªæœç´¢æ–‡ä»¶åçš„å·¥å…·

**æ­¥éª¤**ï¼š

#### 1. åˆ›å»ºå·¥å…·æ–‡ä»¶

```python
# src/kimi_cli/tools/file/find_files.py
from pathlib import Path
from kosong import Tool
from kimi_cli.soul.agent import Runtime

class FindFiles(Tool):
    """æœç´¢æ–‡ä»¶å"""

    @property
    def description(self) -> str:
        return """
        Search for files by name pattern.

        Parameters:
        - pattern (str): File name pattern (supports wildcards like *.py)
        - max_results (int): Maximum number of results to return

        Returns:
        List of matching file paths.
        """

    @property
    def parameters(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "pattern": {
                    "type": "string",
                    "description": "File name pattern (e.g., '*.py', 'test_*.json')",
                },
                "max_results": {
                    "type": "integer",
                    "default": 50,
                    "description": "Maximum number of results",
                },
            },
            "required": ["pattern"],
        }

    async def _prepare(self, runtime: Runtime) -> None:
        self.work_dir = runtime.work_dir

    async def _execute(self, pattern: str, max_results: int = 50) -> dict:
        matches = []

        # é€’å½’æœç´¢
        for path in self.work_dir.rglob(pattern):
            if path.is_file():
                relative_path = path.relative_to(self.work_dir)
                matches.append(str(relative_path))

                if len(matches) >= max_results:
                    break

        return {
            "pattern": pattern,
            "count": len(matches),
            "files": matches,
        }
```

#### 2. æ·»åŠ åˆ° Agent é…ç½®

ç¼–è¾‘ `src/kimi_cli/agents/default/agent.yaml`:

```yaml
tools:
  # ... ç°æœ‰å·¥å…·
  - "kimi_cli.tools.file:FindFiles"
```

#### 3. æµ‹è¯•å·¥å…·

```python
# tests/test_find_files.py
import pytest
from pathlib import Path
from kimi_cli.tools.file.find_files import FindFiles
from kimi_cli.soul.agent import Runtime
from kaos.local import LocalKaos

@pytest.mark.asyncio
async def test_find_files(tmp_path: Path):
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    (tmp_path / "test1.py").touch()
    (tmp_path / "test2.py").touch()
    (tmp_path / "other.txt").touch()

    # åˆå§‹åŒ–å·¥å…·
    runtime = Runtime(
        kaos=LocalKaos(),
        work_dir=tmp_path,
    )
    tool = FindFiles()
    await tool._prepare(runtime)

    # æ‰§è¡Œæœç´¢
    result = await tool._execute(pattern="*.py")

    # éªŒè¯ç»“æœ
    assert result["count"] == 2
    assert "test1.py" in result["files"]
    assert "test2.py" in result["files"]
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
pytest tests/test_find_files.py -v
```

### 6.2 ä»»åŠ¡ 2ï¼šæ·»åŠ ä¸€ä¸ªå­ Agent - æ•°æ®åˆ†æåŠ©æ‰‹

**ç›®æ ‡**ï¼šåˆ›å»ºä¸€ä¸ªä¸“é—¨å¤„ç†æ•°æ®åˆ†æä»»åŠ¡çš„å­ Agent

**æ­¥éª¤**ï¼š

#### 1. åˆ›å»º Agent é…ç½®

```yaml
# src/kimi_cli/agents/data_analyst/agent.yaml
version: 1
agent:
  name: "Data Analyst"
  system_prompt_path: ./system.md
  tools:
    - "kimi_cli.tools.file:ReadFile"
    - "kimi_cli.tools.file:WriteFile"
    - "kimi_cli.tools.shell:Shell"  # å¯ä»¥è¿è¡Œ Python è„šæœ¬
```

#### 2. ç¼–å†™ç³»ç»Ÿæç¤ºè¯

```markdown
# src/kimi_cli/agents/data_analyst/system.md

You are a Data Analyst Agent specialized in data analysis tasks.

Your capabilities:
- Read and analyze CSV, JSON, and other data files
- Write Python scripts for data processing
- Use pandas, numpy, matplotlib for analysis
- Generate visualizations and reports

When given a data analysis task:
1. First, read the data file to understand its structure
2. Write a Python script to perform the analysis
3. Execute the script using the Shell tool
4. Summarize the findings

Always provide clear explanations of your analysis steps.
```

#### 3. åœ¨ä¸» Agent ä¸­æ³¨å†Œ

ç¼–è¾‘ `src/kimi_cli/agents/default/agent.yaml`:

```yaml
subagents:
  coder:
    path: ./sub.yaml
    description: "Good at general software engineering tasks."

  data_analyst:
    path: ../data_analyst/agent.yaml
    description: "Specialized in data analysis and visualization tasks."
```

#### 4. ä½¿ç”¨å­ Agent

ç°åœ¨ä¸» Agent å¯ä»¥å§”æ´¾ä»»åŠ¡ç»™æ•°æ®åˆ†æ Agentï¼š

```
User: Analyze the sales data in sales.csv and create a visualization

Main Agent: I'll delegate this to the Data Analyst.
[Calls Task tool with subagent="data_analyst"]

Data Analyst:
1. Reading sales.csv...
2. Writing analysis script...
3. Running script...
4. Summary: ...
```

### 6.3 ä»»åŠ¡ 3ï¼šæ·»åŠ æ–°çš„ LLM æä¾›å•†

**ç›®æ ‡**ï¼šæ·»åŠ å¯¹ Google Gemini çš„æ”¯æŒ

**æ­¥éª¤**ï¼š

#### 1. å®‰è£…ä¾èµ–

```bash
uv add google-generativeai
```

#### 2. å®ç°æä¾›å•†

```python
# src/kimi_cli/llm.py

import google.generativeai as genai

# æ·»åŠ åˆ° LLMProviderType æšä¸¾
class LLMProviderType(str, Enum):
    # ... ç°æœ‰æä¾›å•†
    GOOGLE_GEMINI = "google_gemini"

# åœ¨ create_llm å‡½æ•°ä¸­æ·»åŠ 
def create_llm(provider_type: str, model: dict, ...) -> LLM:
    if provider_type == LLMProviderType.GOOGLE_GEMINI:
        genai.configure(api_key=model.api_key)
        return GeminiLLM(
            model_name=model.name,
            max_tokens=model.max_context_size,
        )
    # ... å…¶ä»–æä¾›å•†

# å®ç° GeminiLLM ç±»
class GeminiLLM:
    def __init__(self, model_name: str, max_tokens: int):
        self.model = genai.GenerativeModel(model_name)
        self.max_tokens = max_tokens

    async def chat(self, messages: list[dict], tools: list = None) -> dict:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        gemini_messages = self._convert_messages(messages)

        # è°ƒç”¨ Gemini
        response = await self.model.generate_content_async(
            gemini_messages,
            tools=tools,
        )

        # è§£æå“åº”
        return self._parse_response(response)
```

#### 3. æ›´æ–°é…ç½®

```json
// ~/.kimi/config.json
{
  "providers": {
    "google": {
      "type": "google_gemini",
      "api_key": "your-api-key"
    }
  },
  "models": {
    "gemini-pro": {
      "provider": "google",
      "name": "gemini-pro",
      "max_context_size": 30000
    }
  }
}
```

#### 4. ä½¿ç”¨æ–°æ¨¡å‹

```bash
kimi --model gemini-pro
```

---

## 7. é«˜çº§ä¸»é¢˜

### 7.1 å¤š Agent åä½œ

**åœºæ™¯**ï¼šä¸» Agent å°†å¤æ‚ä»»åŠ¡åˆ†è§£ç»™å¤šä¸ªå­ Agent

**å®ç°**ï¼š

```python
# src/kimi_cli/tools/multiagent/task.py
class Task(Tool):
    async def _execute(
        self,
        subagent: str,
        prompt: str,
    ) -> str:
        # 1. è·å–å­ Agent
        subagent_instance = self.runtime.subagents[subagent]

        # 2. åˆ›å»ºç‹¬ç«‹çš„ä¸Šä¸‹æ–‡
        subagent_context = Context()

        # 3. åˆ›å»º Soul
        subagent_soul = KimiSoul(
            agent=subagent_instance,
            context=subagent_context,
            runtime=self.runtime,
        )

        # 4. æ‰§è¡Œä»»åŠ¡
        result = await subagent_soul.step(prompt)

        return result.text
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
# ä¸» Agent æ¨ç†
"""
User wants to build a web application.
This is complex, I'll delegate:
1. Backend API â†’ coder subagent
2. Frontend UI â†’ coder subagent
3. Database schema â†’ coder subagent
"""

# è°ƒç”¨å·¥å…·
await Task(subagent="coder", prompt="Design a REST API for user management")
await Task(subagent="coder", prompt="Create a React frontend with login page")
```

### 7.2 å†å²å‹ç¼©ç­–ç•¥

**é—®é¢˜**ï¼šé•¿å¯¹è¯è¶…å‡º LLM ä¸Šä¸‹æ–‡é™åˆ¶

**ç­–ç•¥ 1ï¼šæ»‘åŠ¨çª—å£**

```python
class SlidingWindowContext(Context):
    def compact(self, window_size: int = 10):
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€è¿‘ N æ¡
        self.messages = [
            self.messages[0],  # ç³»ç»Ÿæ¶ˆæ¯
            *self.messages[-window_size:],  # æœ€è¿‘ N æ¡
        ]
```

**ç­–ç•¥ 2ï¼šLLM å‹ç¼©**

```python
class LLMCompactionContext(Context):
    async def compact(self, llm: LLMProvider):
        # æå–éœ€è¦å‹ç¼©çš„æ¶ˆæ¯
        old_messages = self.messages[1:-10]

        # è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
        summary = await llm.chat([
            {"role": "system", "content": COMPACTION_PROMPT},
            *old_messages,
        ])

        # æ›¿æ¢ä¸ºæ‘˜è¦
        self.messages = [
            self.messages[0],
            {"role": "system", "content": f"Previous conversation summary:\n{summary}"},
            *self.messages[-10:],
        ]
```

**ç­–ç•¥ 3ï¼šå…³é”®ä¿¡æ¯æå–**

```python
class KeyInfoContext(Context):
    async def compact(self, llm: LLMProvider):
        # æå–å…³é”®ä¿¡æ¯
        key_info = await llm.chat([
            {
                "role": "system",
                "content": "Extract key information from the conversation: "
                          "file paths, variable names, decisions made, etc."
            },
            *self.messages[1:-10],
        ])

        self.messages = [
            self.messages[0],
            {"role": "system", "content": f"Key context:\n{key_info}"},
            *self.messages[-10:],
        ]
```

### 7.3 é”™è¯¯å¤„ç†å’Œé‡è¯•

**ä½¿ç”¨ tenacity å®ç°æ™ºèƒ½é‡è¯•**ï¼š

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

class KimiSoul:
    @retry(
        stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯• 3 æ¬¡
        wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿
        retry=retry_if_exception_type((APIError, TimeoutError)),  # ä»…é‡è¯•ç‰¹å®šé”™è¯¯
    )
    async def _call_llm(self) -> dict:
        return await self.llm.chat(
            messages=self.context.messages,
            tools=self.agent.tool_schemas,
        )
```

**é”™è¯¯åˆ†ç±»å¤„ç†**ï¼š

```python
class KimiSoul:
    async def step(self, user_message: str = None):
        try:
            return await self._step_impl(user_message)
        except APIError as e:
            # API é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
            if "rate_limit" in str(e):
                await asyncio.sleep(5)
                return await self.step(user_message)
            else:
                raise
        except ToolExecutionError as e:
            # å·¥å…·æ‰§è¡Œé”™è¯¯ï¼ˆè¿”å›é”™è¯¯ä¿¡æ¯ç»™ LLMï¼‰
            self.context.add_tool_error(str(e))
            return await self.step()  # ç»§ç»­å¾ªç¯
        except ValidationError as e:
            # å‚æ•°éªŒè¯é”™è¯¯ï¼ˆä¸å¯é‡è¯•ï¼‰
            return {"error": f"Invalid parameters: {e}"}
```

### 7.4 æ€§èƒ½ä¼˜åŒ–

#### 7.4.1 å¹¶è¡Œå·¥å…·è°ƒç”¨

```python
async def _execute_tools(self, tool_calls: list[dict]) -> list[str]:
    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·
    tasks = [
        self._execute_single_tool(tc)
        for tc in tool_calls
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # å¤„ç†å¼‚å¸¸
    return [
        str(result) if not isinstance(result, Exception) else f"Error: {result}"
        for result in results
    ]
```

#### 7.4.2 ç¼“å­˜

```python
from functools import lru_cache

class ReadFile(Tool):
    @lru_cache(maxsize=100)
    async def _read_cached(self, file_path: str) -> str:
        return await self.kaos.readtext(file_path)

    async def _execute(self, file_path: str, **kwargs) -> str:
        content = await self._read_cached(file_path)
        # ... å¤„ç†å†…å®¹
```

#### 7.4.3 æµå¼è¾“å‡º

```python
class KimiSoul:
    async def step_streaming(self, user_message: str):
        # æµå¼è°ƒç”¨ LLM
        async for chunk in self.llm.chat_stream(self.context.messages):
            if chunk.type == "text":
                yield {"type": "text", "data": chunk.text}
            elif chunk.type == "tool_call":
                yield {"type": "tool_call", "data": chunk.tool_call}
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 ç³»ç»Ÿæç¤ºè¯è®¾è®¡

#### åŸåˆ™

1. **æ¸…æ™°æ˜ç¡®**ï¼šæ˜ç¡® Agent çš„è§’è‰²å’Œèƒ½åŠ›
2. **ç»“æ„åŒ–**ï¼šä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡ä¿¡æ¯
3. **ç¤ºä¾‹é©±åŠ¨**ï¼šæä¾›å…·ä½“çš„ä½¿ç”¨ç¤ºä¾‹
4. **çº¦æŸæ˜ç¡®**ï¼šè¯´æ˜ Agent çš„é™åˆ¶

#### ç¤ºä¾‹

```markdown
# System Prompt Template

## Role
You are ${AGENT_NAME}, an AI assistant specialized in ${DOMAIN}.

## Capabilities
You have access to the following tools:
- Tool1: Description
- Tool2: Description

## Guidelines

### When to use tools
- Use ReadFile when you need to examine code
- Use Shell when you need to run commands
- Use Task when you need specialized help

### Best practices
1. Always read files before editing
2. Test changes before committing
3. Provide clear explanations

## Constraints
- Only access files within ${WORK_DIR}
- Always ask before destructive operations
- Respect user preferences

## Examples

### Example 1: Reading a file
User: "What's in main.py?"
Assistant: I'll read the file.
[Calls ReadFile tool]
The file contains...

### Example 2: Running tests
User: "Run the tests"
Assistant: I'll execute the test suite.
[Calls Shell tool with "pytest"]
```

### 8.2 å·¥å…·è®¾è®¡åŸåˆ™

#### 1. å•ä¸€èŒè´£

```python
# âŒ ä¸å¥½ï¼šä¸€ä¸ªå·¥å…·åšå¤ªå¤šäº‹
class FileManager(Tool):
    def _execute(self, action: str, path: str, content: str = None):
        if action == "read":
            return self._read(path)
        elif action == "write":
            return self._write(path, content)
        elif action == "delete":
            return self._delete(path)

# âœ… å¥½ï¼šæ¯ä¸ªå·¥å…·åªåšä¸€ä»¶äº‹
class ReadFile(Tool):
    def _execute(self, path: str): ...

class WriteFile(Tool):
    def _execute(self, path: str, content: str): ...

class DeleteFile(Tool):
    def _execute(self, path: str): ...
```

#### 2. æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

```python
class ReadFile(Tool):
    async def _execute(self, file_path: str) -> str:
        try:
            return await self.kaos.readtext(file_path)
        except FileNotFoundError:
            raise ToolExecutionError(
                f"File not found: {file_path}\n"
                f"Working directory: {self.work_dir}\n"
                f"Available files: {list(self.work_dir.iterdir())}"
            )
        except PermissionError:
            raise ToolExecutionError(
                f"Permission denied: {file_path}\n"
                f"Please check file permissions."
            )
```

#### 3. å‚æ•°éªŒè¯

```python
class WriteFile(Tool):
    async def _execute(self, file_path: str, content: str) -> str:
        # éªŒè¯è·¯å¾„
        if ".." in file_path:
            raise ValidationError("Path traversal not allowed")

        # éªŒè¯å†…å®¹å¤§å°
        if len(content) > 1_000_000:  # 1 MB
            raise ValidationError("Content too large (max 1 MB)")

        # æ‰§è¡Œå†™å…¥
        await self.kaos.writetext(file_path, content)
        return "File written successfully"
```

### 8.3 æµ‹è¯•ç­–ç•¥

#### å•å…ƒæµ‹è¯•

```python
@pytest.mark.asyncio
async def test_read_file_success(tmp_path):
    # Arrange
    test_file = tmp_path / "test.txt"
    test_file.write_text("Hello, world!")

    runtime = Runtime(kaos=LocalKaos(), work_dir=tmp_path)
    tool = ReadFile()
    await tool._prepare(runtime)

    # Act
    result = await tool._execute("test.txt")

    # Assert
    assert "Hello, world!" in result

@pytest.mark.asyncio
async def test_read_file_not_found(tmp_path):
    # Arrange
    runtime = Runtime(kaos=LocalKaos(), work_dir=tmp_path)
    tool = ReadFile()
    await tool._prepare(runtime)

    # Act & Assert
    with pytest.raises(ToolExecutionError, match="File not found"):
        await tool._execute("nonexistent.txt")
```

#### é›†æˆæµ‹è¯•

```python
@pytest.mark.asyncio
async def test_agent_workflow(mock_llm, tmp_path):
    # è®¾ç½® Mock LLM
    mock_llm.set_responses([
        {"text": "I'll read the file", "tool_calls": [
            {"name": "ReadFile", "parameters": {"file_path": "test.txt"}}
        ]},
        {"text": "The file contains: Hello", "tool_calls": []},
    ])

    # åˆ›å»º Agent
    agent = create_test_agent(llm=mock_llm, work_dir=tmp_path)

    # æ‰§è¡Œ
    response = await agent.step("What's in test.txt?")

    # éªŒè¯
    assert "Hello" in response.text
    assert mock_llm.call_count == 2
```

### 8.4 å®‰å…¨æ€§è€ƒè™‘

#### 1. è·¯å¾„éå†ä¿æŠ¤

```python
def validate_path(base_dir: Path, user_path: str) -> Path:
    # è§£æç»å¯¹è·¯å¾„
    full_path = (base_dir / user_path).resolve()

    # æ£€æŸ¥æ˜¯å¦åœ¨ base_dir å†…
    if not full_path.is_relative_to(base_dir):
        raise SecurityError("Path traversal attempt detected")

    return full_path
```

#### 2. å‘½ä»¤æ³¨å…¥ä¿æŠ¤

```python
import shlex

class Shell(Tool):
    DANGEROUS_COMMANDS = ["rm -rf", "dd", "mkfs", ":(){ :|:& };:"]

    async def _execute(self, command: str) -> str:
        # æ£€æŸ¥å±é™©å‘½ä»¤
        for dangerous in self.DANGEROUS_COMMANDS:
            if dangerous in command:
                raise SecurityError(f"Dangerous command detected: {dangerous}")

        # ä½¿ç”¨ shlex è½¬ä¹‰
        safe_command = shlex.quote(command)

        # æ‰§è¡Œ
        return await self._run_shell(safe_command)
```

#### 3. API Key ä¿æŠ¤

```python
from pydantic import SecretStr

class Config:
    api_key: SecretStr  # è‡ªåŠ¨éšè—æ‰“å°

    def to_dict(self) -> dict:
        return {
            "api_key": "***"  # ä¸æš´éœ²çœŸå®å€¼
        }
```

### 8.5 è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
from loguru import logger

# é…ç½® loguru
logger.add(
    "kimi.log",
    level="DEBUG",
    rotation="10 MB",
    format="{time} {level} {message}",
)

# åœ¨ä»£ç ä¸­ä½¿ç”¨
logger.debug(f"Calling LLM with {len(messages)} messages")
logger.info(f"Tool {tool_name} executed successfully")
logger.error(f"Error in tool execution: {error}")
```

#### 2. ä¿å­˜è°ƒè¯•ä¿¡æ¯

```python
class KimiSoul:
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.debug_log = []

    async def step(self, user_message: str):
        if self.debug:
            self.debug_log.append({
                "timestamp": datetime.now(),
                "user_message": user_message,
                "context_size": len(self.context.messages),
            })

        # ... æ‰§è¡Œé€»è¾‘

        if self.debug:
            with open("debug.json", "w") as f:
                json.dump(self.debug_log, f, indent=2)
```

#### 3. ä½¿ç”¨ Replay åŠŸèƒ½

```python
# ä¿å­˜ä¼šè¯
session.save("session.json")

# é‡æ”¾ä¼šè¯
session = Session.load("session.json")
for message in session.messages:
    print(f"{message['role']}: {message['content']}")
```

---

## æ€»ç»“

### ä½ å­¦åˆ°äº†ä»€ä¹ˆ

é€šè¿‡æœ¬æŒ‡å—ï¼Œä½ åº”è¯¥æŒæ¡äº†ï¼š

1. **Agent ç³»ç»Ÿæ¶æ„**
   - LLM é›†æˆ
   - å·¥å…·ç³»ç»Ÿ
   - ä¸Šä¸‹æ–‡ç®¡ç†
   - æ‰§è¡Œå¼•æ“

2. **Kimi CLI çš„å®ç°**
   - KAOS æ–‡ä»¶ç³»ç»ŸæŠ½è±¡
   - Soul æ‰§è¡Œå¼•æ“
   - å¤š UI æ¨¡å¼
   - Agent é…ç½®ç³»ç»Ÿ

3. **å®æˆ˜æŠ€èƒ½**
   - æ·»åŠ æ–°å·¥å…·
   - åˆ›å»ºå­ Agent
   - é›†æˆæ–° LLM æä¾›å•†
   - æµ‹è¯•å’Œè°ƒè¯•

4. **é«˜çº§ä¸»é¢˜**
   - å¤š Agent åä½œ
   - å†å²å‹ç¼©
   - æ€§èƒ½ä¼˜åŒ–
   - å®‰å…¨æ€§

### ä¸‹ä¸€æ­¥

1. **æ·±å…¥æºç **
   - é˜…è¯» `src/kimi_cli/soul/kimisoul.py`
   - ç ”ç©¶å·¥å…·å®ç°
   - ç†è§£ Agent åŠ è½½æµç¨‹

2. **åŠ¨æ‰‹å®è·µ**
   - å®Œæˆä¸Šé¢çš„ 3 ä¸ªå®æˆ˜ä»»åŠ¡
   - åˆ›å»ºè‡ªå·±çš„ Agent
   - è´¡çŒ®ä»£ç åˆ°é¡¹ç›®

3. **æ¢ç´¢ç”Ÿæ€**
   - äº†è§£ MCP åè®®
   - ç ”ç©¶ ACP é›†æˆ
   - æ¢ç´¢å…¶ä»– Agent æ¡†æ¶ï¼ˆLangChainã€AutoGPTï¼‰

4. **æ„å»ºè‡ªå·±çš„ Agent**
   - æ ¹æ®éœ€æ±‚è®¾è®¡æ¶æ„
   - å®ç°æ ¸å¿ƒåŠŸèƒ½
   - æµ‹è¯•å’Œä¼˜åŒ–

### æ¨èèµ„æº

- [Kimi CLI æ–‡æ¡£](https://www.kimi.com/coding/docs/kimi-cli.html)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [LangChain](https://python.langchain.com/)
- [Anthropic Claude](https://www.anthropic.com/claude)
- [Model Context Protocol](https://modelcontextprotocol.io/)

---

**æ­å–œä½ å®Œæˆäº†è¿™ä¸ªå…¨é¢çš„æŒ‡å—ï¼ç°åœ¨ä½ å·²ç»æŒæ¡äº†æ„å»º Coding Agent çš„æ ¸å¿ƒçŸ¥è¯†ã€‚**

Happy coding! ğŸš€
